# -*- coding: utf-8 -*-
"""depremanalizi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vSpqXeHKmfBFeQraVkRHru-WCkFdZHMg
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

file_path = "turkey_earthquakes(1915-2023_may).csv"

try:
    df = pd.read_csv(file_path, encoding='ISO-8859-1')
except UnicodeDecodeError:
    try:
        df = pd.read_csv(file_path, encoding='latin-1')
    except UnicodeDecodeError:
        print("ISO-8859-1 veya latin-1 kodlaması ile dosyanın kodu çözülemedi")
        print("Lütfen dosyanızla eşleşen farklı bir kodlama deneyin.")

df.shape

df.describe()

print(df.info())

print(df.isnull().sum())

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# 1. Giriş değişkenleri
features = ['xM', 'MD', 'ML', 'Mb', 'Ms']

# 2. Eğitim verisi: Mw değeri olanlar
train_df = df[df['Mw'].notnull()]
X_train = train_df[features]
y_train = train_df['Mw']

# 3. Tahmin verisi: Mw eksik olanlar
test_df = df[df['Mw'].isnull()]
X_test = test_df[features]

# 4. Eksik değer içeren satırlar varsa onları at (model eğitiminden önce)
X_train = X_train.dropna()
y_train = y_train.loc[X_train.index]
X_test = X_test.dropna()

# 5. Model eğitimi
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 6. Mw tahmini
predicted_mw = model.predict(X_test)

# 7. Tahmin edilen değerleri orijinal df’ye geri yaz
df.loc[X_test.index, 'Mw'] = predicted_mw
df['Mw_is_imputed'] = df['Mw'].isnull().astype(int)

print("✅ Eksik Mw değerleri tahmin edilip dolduruldu.")

print(df.isnull().sum())

print(df.columns.tolist())

df['Olus tarihi'] = pd.to_datetime(df['Olus tarihi'])

# Gerekli sütunları seçme
df = df[['Olus tarihi', 'Olus zamani', 'Enlem', 'Boylam', 'Derinlik', 'Mw', 'ML', 'xM', 'Tip', 'Yer']]

# Eksik büyüklükler için Mw yerine ML veya xM kullanma
df['Büyüklük'] = df['Mw'].fillna(df['ML']).fillna(df['xM'])

# Tarih ve zamanı birleştirme
# Convert both columns to string type before concatenation
df['Timestamp'] = pd.to_datetime(df['Olus tarihi'].astype(str) + ' ' + df['Olus zamani'].astype(str))
df.drop(['Olus tarihi', 'Olus zamani', 'Mw', 'ML', 'xM'], axis=1, inplace=True)

print(df.info())

df = df.rename(columns={'Timestamp': 'Oluş Zamanı'})

print(df.info())

df.to_csv('deprem_verisi.csv', encoding='utf-8-sig', index=False)

sns.boxplot(x=df['Büyüklük'])
plt.show()

import numpy as np

# Büyüklük verisine log10 dönüşümü (Richter ölçeği zaten logaritmik olduğu için)
df['Log_Büyüklük'] = np.log10(df['Büyüklük'] + 0.1)  # +0.1 sıfır değerlerini önlemek için

# Orijinal ve log dönüşümlü verinin karşılaştırılması
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['Büyüklük'], bins=30, kde=True)
plt.title('Orijinal Büyüklük Dağılımı')

plt.subplot(1, 2, 2)
sns.histplot(df['Log_Büyüklük'], bins=30, kde=True)
plt.title('Log10 Dönüşümlü Dağılım')
plt.show()

# 1. Büyüklük sınıfları oluşturma (artçı/öncü ayırımı için)
df['Büyüklük_Sınıfı'] = pd.cut(df['Büyüklük'],
                               bins=[0, 3, 5, 10],
                               labels=['Küçük', 'Orta', 'Büyük'])

# 2. Standardizasyon (log dönüşümü sonrası)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df['Log_Büyüklük_Std'] = scaler.fit_transform(df[['Log_Büyüklük']])

plt.figure(figsize=(14, 6))

# Orijinal ve log dönüşümlü dağılım karşılaştırması
plt.subplot(1, 2, 1)
sns.histplot(df['Büyüklük'], bins=30, kde=True, color='skyblue')
plt.title('Orijinal Büyüklük Dağılımı\n(Richter Ölçeği)')
plt.xlabel('Büyüklük (Richter)')

plt.subplot(1, 2, 2)
sns.histplot(df['Log_Büyüklük'], bins=30, kde=True, color='salmon')
plt.title('Log10 Dönüşümlü Dağılım\n(log₁₀(Richter + 0.01))')
plt.xlabel('Log10(Büyüklük)')

plt.tight_layout()
plt.show()

# 1. Büyüklük kategorileri oluşturma
df['Büyüklük_Kategori'] = pd.cut(df['Büyüklük'],
                                bins=[0, 3, 5, np.inf],
                                labels=['Hafif', 'Orta', 'Şiddetli'])

# 2. Log dönüşümü + standardizasyon
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df['Log_Büyüklük_Std'] = scaler.fit_transform(df[['Log_Büyüklük']])

plt.figure(figsize=(16, 6))

# Orijinal dağılım
plt.subplot(1, 3, 1)
sns.violinplot(x=df['Büyüklük'], color='lightblue')
plt.title('Orijinal Büyüklük Dağılımı\n(Richter Ölçeği)')

# Log dönüşümlü dağılım
plt.subplot(1, 3, 2)
sns.violinplot(x=df['Log_Büyüklük'], color='lightgreen')
plt.title('Log10 Dönüşümlü Dağılım')

# Kümülatif dağılım
plt.subplot(1, 3, 3)
sns.ecdfplot(data=df, x='Büyüklük', complementary=True)
plt.yscale('log')
plt.title('Kümülatif Dağılım (Log-Log)')
plt.tight_layout()

from geopy.distance import geodesic

def label_aftershocks(df, mainshock_mag_threshold=6.0, time_window_days=7, distance_km=50):
    df = df.sort_values("Oluş Zamanı").reset_index(drop=True)
    df['is_aftershock'] = 0  # Başlangıçta tüm satırlar artçı değil

    for i, row in df.iterrows():
        if row['Büyüklük'] >= mainshock_mag_threshold:
            mainshock_time = row['Oluş Zamanı']
            mainshock_loc = (row['Enlem'], row['Boylam'])

            time_window = (df['Oluş Zamanı'] > mainshock_time) & \
                          (df['Oluş Zamanı'] <= mainshock_time + pd.Timedelta(days=time_window_days))

            for j in df[time_window].index:
                aftershock_loc = (df.loc[j, 'Enlem'], df.loc[j, 'Boylam'])
                distance = geodesic(mainshock_loc, aftershock_loc).km

                if distance <= distance_km and df.loc[j, 'Büyüklük'] < row['Büyüklük']:
                    df.at[j, 'is_aftershock'] = 1

    return df

df = label_aftershocks(df)
df['is_aftershock'].value_counts()

from geopy.distance import geodesic

def ozellikleri_olustur(df):
    df = df.sort_values("Oluş Zamanı").reset_index(drop=True)

    df['Unix Zamanı'] = df['Oluş Zamanı'].astype(np.int64) // 10**9
    df['Saat'] = df['Oluş Zamanı'].dt.hour
    df['Haftanın Günü'] = df['Oluş Zamanı'].dt.dayofweek

    zaman_farki = [0]
    mesafe_farki = [0]

    for i in range(1, len(df)):
        simdi_zaman = df.loc[i, 'Oluş Zamanı']
        onceki_zaman = df.loc[i-1, 'Oluş Zamanı']
        zaman_farki.append((simdi_zaman - onceki_zaman).total_seconds())

        simdi_konum = (df.loc[i, 'Enlem'], df.loc[i, 'Boylam'])
        onceki_konum = (df.loc[i-1, 'Enlem'], df.loc[i-1, 'Boylam'])
        mesafe_farki.append(geodesic(simdi_konum, onceki_konum).km)

    df['Önceki Zaman Farkı (sn)'] = zaman_farki
    df['Önceki Mesafe (km)'] = mesafe_farki

    return df

df = ozellikleri_olustur(df)
df[['Büyüklük', 'Derinlik', 'Saat', 'Haftanın Günü', 'Önceki Zaman Farkı (sn)', 'Önceki Mesafe (km)']].head()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

ozellikler = ['Büyüklük', 'Derinlik', 'Saat', 'Haftanın Günü',
              'Önceki Zaman Farkı (sn)', 'Önceki Mesafe (km)']

X = df[ozellikler]
y = df['is_aftershock']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

model = RandomForestClassifier(random_state=42, class_weight='balanced')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, accuracy_score

def modeli_test_et(model, X_train, y_train, X_test, y_test, model_adi):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"🔹 {model_adi} Sonuçları:")
    print("Accuracy:", round(accuracy_score(y_test, y_pred), 3))
    print(classification_report(y_test, y_pred, target_names=["Artçı Değil", "Artçı"]))
    print("-" * 60)

modeller = {
    "Random Forest": RandomForestClassifier(random_state=42, class_weight='balanced'),
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "XGBoost": XGBClassifier(random_state=42, scale_pos_weight=12),  # sınıf dengesizliği için
    "LightGBM": LGBMClassifier(random_state=42, class_weight='balanced')
}

for isim, model in modeller.items():
    modeli_test_et(model, X_train, y_train, X_test, y_test, isim)

import joblib
from xgboost import XGBClassifier # XGBoost sınıfını yeniden import et

# Modelleri test etme döngüsü zaten çalıştı.
# Bu döngünün sonunda `model` değişkeni en son test edilen modeli (LightGBM) içerir.
# XGBoost modelinin eğitilmiş halini kaydetmek istiyorsanız,
# ya döngü içinde kaydedin ya da döngü dışında XGBoost modeline tekrar erişin.

# En basit çözüm, döngü bittikten sonra XGBoost modeline tekrar erişmek ve kaydetmektir.
# Modeller sözlüğünden XGBoost modelini alın
xgboost_model_instance = modeller["XGBoost"]

# modeli_test_et fonksiyonu her döngüde modeli eğitir.
# Döngü zaten tamamlandığı için, `xgboost_model_instance` değişkeni eğitilmiş XGBoost modelini tutar.

# Şimdi bu eğitilmiş XGBoost modelini kaydedin
joblib.dump(xgboost_model_instance, 'xgboost_smote_model.pkl') # Doğru değişken adı kullanıldı

print("XGBoost modeli başarıyla 'xgboost_smote_model.pkl' dosyasına kaydedildi.")

# İstediğin zaman geri yüklemek için:
# loaded_model = joblib.load('xgboost_smote_model.pkl')

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import pandas as pd # pandas'ı tekrar import et

# Özellik ve hedef değişkenleri ayır
ozellikler = ['Büyüklük', 'Derinlik', 'Saat', 'Haftanın Günü',
              'Önceki Zaman Farkı (sn)', 'Önceki Mesafe (km)'] # Özellik listesini yeniden tanımla

# Sadece kullanılacak sütunlarda eksik değer kontrolü yap
df_cleaned = df.dropna(subset=ozellikler + ['is_aftershock']).copy() # Eksik değer içeren satırları at ve kopya oluştur

# Temizlenmiş DataFrame'den özellikleri ve hedef değişkeni ayır
X = df_cleaned[ozellikler]
y = df_cleaned["is_aftershock"]

# Veriyi eğitim ve test olarak ayır
# stratify=y, sınıf dağılımını korumak için hala önemlidir
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# SMOTE uygulama
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Sınıf dağılımını kontrol edelim
print("Önce (Eğitim Seti):", y_train.value_counts())
print("Sonra (SMOTE Sonrası Eğitim Seti):", y_train_resampled.value_counts())

# Model eğitimi ve değerlendirmesi için XGBoost ve diğer modeller
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix

params = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5, 7],
    "learning_rate": [0.01, 0.1, 0.2],
    "subsample": [0.7, 1.0],
    "colsample_bytree": [0.7, 1.0]
}

# XGBoost modelini SMOTE sonrası eğitim verisiyle eğit
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

grid = GridSearchCV(xgb, params, scoring='f1', cv=3, n_jobs=-1)
grid.fit(X_train_resampled, y_train_resampled)

print("En iyi parametreler:", grid.best_params_)

best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

print("\nGridSearchCV (SMOTE + XGBoost) Sonuçları (Test Seti):")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# Diğer modelleri de SMOTE sonrası eğitim verisiyle test etmek isterseniz:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score

# Daha önce tanımlanmış modeli_test_et fonksiyonunu kullanabilirsiniz
# Ancak bu fonksiyon SMOTE'yi otomatik yapmıyor, bu yüzden manuel olarak
# X_train_resampled ve y_train_resampled ile modelleri yeniden eğitmeniz gerekir.

print("\nDiğer Modellerin Sonuçları (SMOTE Sonrası Eğitim ile):")
modeller_smote = {
    "Random Forest": RandomForestClassifier(random_state=42, class_weight='balanced'),
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "KNN": KNeighborsClassifier(n_neighbors=5), # KNN class_weight'i desteklemez, scale_pos_weight XGBoost/LightGBM'e özgüdür
    "LightGBM": LGBMClassifier(random_state=42, class_weight='balanced')
}

for isim, model in modeller_smote.items():
    model.fit(X_train_resampled, y_train_resampled) # Modeli SMOTElenmiş veriyle eğit
    y_pred_other = model.predict(X_test)
    print(f"🔹 {isim} Sonuçları (SMOTE Eğitim):")
    print("Accuracy:", round(accuracy_score(y_test, y_pred_other), 3))
    print(classification_report(y_test, y_pred_other, target_names=["Artçı Değil", "Artçı"]))
    print("-" * 60)

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Temel XGBoost modeli
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train_resampled, y_train_resampled)

# Test verisinde tahmin
y_pred_xgb = xgb.predict(X_test)

# Sonuçlar
print("🔹 XGBoost Sonuçları (SMOTE Eğitim):")
print(classification_report(y_test, y_pred_xgb, target_names=["Artçı Değil", "Artçı"]))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
grid = GridSearchCV(estimator=xgb_model,
                    param_grid=param_grid,
                    scoring='f1',
                    cv=3,
                    n_jobs=-1,
                    verbose=1)

grid.fit(X_train_resampled, y_train_resampled)

from sklearn.metrics import classification_report, confusion_matrix

best_xgb = grid.best_estimator_
y_pred_best = best_xgb.predict(X_test)

print("🔧 En iyi parametreler:", grid.best_params_)
print("🔹 XGBoost (GridSearch) Sonuçları:")
print(classification_report(y_test, y_pred_best, target_names=["Artçı Değil", "Artçı"]))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))

from sklearn.metrics import classification_report, confusion_matrix

y_proba = best_xgb.predict_proba(X_test)[:, 1]

for thresh in [0.5, 0.4, 0.35, 0.3]:
    print(f"\n🎯 Eşik: {thresh}")
    y_pred_thresh = (y_proba >= thresh).astype(int)
    print(classification_report(y_test, y_pred_thresh, target_names=["Artçı Değil", "Artçı"]))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_thresh))
