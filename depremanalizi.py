# -*- coding: utf-8 -*-
"""depremanalizi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vSpqXeHKmfBFeQraVkRHru-WCkFdZHMg
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

file_path = "turkey_earthquakes(1915-2023_may).csv"

try:
    df = pd.read_csv(file_path, encoding='ISO-8859-1')
except UnicodeDecodeError:
    try:
        df = pd.read_csv(file_path, encoding='latin-1')
    except UnicodeDecodeError:
        print("ISO-8859-1 veya latin-1 kodlamasÄ± ile dosyanÄ±n kodu Ã§Ã¶zÃ¼lemedi")
        print("LÃ¼tfen dosyanÄ±zla eÅŸleÅŸen farklÄ± bir kodlama deneyin.")

df.shape

df.describe()

print(df.info())

print(df.isnull().sum())

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# 1. GiriÅŸ deÄŸiÅŸkenleri
features = ['xM', 'MD', 'ML', 'Mb', 'Ms']

# 2. EÄŸitim verisi: Mw deÄŸeri olanlar
train_df = df[df['Mw'].notnull()]
X_train = train_df[features]
y_train = train_df['Mw']

# 3. Tahmin verisi: Mw eksik olanlar
test_df = df[df['Mw'].isnull()]
X_test = test_df[features]

# 4. Eksik deÄŸer iÃ§eren satÄ±rlar varsa onlarÄ± at (model eÄŸitiminden Ã¶nce)
X_train = X_train.dropna()
y_train = y_train.loc[X_train.index]
X_test = X_test.dropna()

# 5. Model eÄŸitimi
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 6. Mw tahmini
predicted_mw = model.predict(X_test)

# 7. Tahmin edilen deÄŸerleri orijinal dfâ€™ye geri yaz
df.loc[X_test.index, 'Mw'] = predicted_mw
df['Mw_is_imputed'] = df['Mw'].isnull().astype(int)

print("âœ… Eksik Mw deÄŸerleri tahmin edilip dolduruldu.")

print(df.isnull().sum())

print(df.columns.tolist())

df['Olus tarihi'] = pd.to_datetime(df['Olus tarihi'])

# Gerekli sÃ¼tunlarÄ± seÃ§me
df = df[['Olus tarihi', 'Olus zamani', 'Enlem', 'Boylam', 'Derinlik', 'Mw', 'ML', 'xM', 'Tip', 'Yer']]

# Eksik bÃ¼yÃ¼klÃ¼kler iÃ§in Mw yerine ML veya xM kullanma
df['BÃ¼yÃ¼klÃ¼k'] = df['Mw'].fillna(df['ML']).fillna(df['xM'])

# Tarih ve zamanÄ± birleÅŸtirme
# Convert both columns to string type before concatenation
df['Timestamp'] = pd.to_datetime(df['Olus tarihi'].astype(str) + ' ' + df['Olus zamani'].astype(str))
df.drop(['Olus tarihi', 'Olus zamani', 'Mw', 'ML', 'xM'], axis=1, inplace=True)

print(df.info())

df = df.rename(columns={'Timestamp': 'OluÅŸ ZamanÄ±'})

print(df.info())

df.to_csv('deprem_verisi.csv', encoding='utf-8-sig', index=False)

sns.boxplot(x=df['BÃ¼yÃ¼klÃ¼k'])
plt.show()

import numpy as np

# BÃ¼yÃ¼klÃ¼k verisine log10 dÃ¶nÃ¼ÅŸÃ¼mÃ¼ (Richter Ã¶lÃ§eÄŸi zaten logaritmik olduÄŸu iÃ§in)
df['Log_BÃ¼yÃ¼klÃ¼k'] = np.log10(df['BÃ¼yÃ¼klÃ¼k'] + 0.1)  # +0.1 sÄ±fÄ±r deÄŸerlerini Ã¶nlemek iÃ§in

# Orijinal ve log dÃ¶nÃ¼ÅŸÃ¼mlÃ¼ verinin karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['BÃ¼yÃ¼klÃ¼k'], bins=30, kde=True)
plt.title('Orijinal BÃ¼yÃ¼klÃ¼k DaÄŸÄ±lÄ±mÄ±')

plt.subplot(1, 2, 2)
sns.histplot(df['Log_BÃ¼yÃ¼klÃ¼k'], bins=30, kde=True)
plt.title('Log10 DÃ¶nÃ¼ÅŸÃ¼mlÃ¼ DaÄŸÄ±lÄ±m')
plt.show()

# 1. BÃ¼yÃ¼klÃ¼k sÄ±nÄ±flarÄ± oluÅŸturma (artÃ§Ä±/Ã¶ncÃ¼ ayÄ±rÄ±mÄ± iÃ§in)
df['BÃ¼yÃ¼klÃ¼k_SÄ±nÄ±fÄ±'] = pd.cut(df['BÃ¼yÃ¼klÃ¼k'],
                               bins=[0, 3, 5, 10],
                               labels=['KÃ¼Ã§Ã¼k', 'Orta', 'BÃ¼yÃ¼k'])

# 2. Standardizasyon (log dÃ¶nÃ¼ÅŸÃ¼mÃ¼ sonrasÄ±)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df['Log_BÃ¼yÃ¼klÃ¼k_Std'] = scaler.fit_transform(df[['Log_BÃ¼yÃ¼klÃ¼k']])

plt.figure(figsize=(14, 6))

# Orijinal ve log dÃ¶nÃ¼ÅŸÃ¼mlÃ¼ daÄŸÄ±lÄ±m karÅŸÄ±laÅŸtÄ±rmasÄ±
plt.subplot(1, 2, 1)
sns.histplot(df['BÃ¼yÃ¼klÃ¼k'], bins=30, kde=True, color='skyblue')
plt.title('Orijinal BÃ¼yÃ¼klÃ¼k DaÄŸÄ±lÄ±mÄ±\n(Richter Ã–lÃ§eÄŸi)')
plt.xlabel('BÃ¼yÃ¼klÃ¼k (Richter)')

plt.subplot(1, 2, 2)
sns.histplot(df['Log_BÃ¼yÃ¼klÃ¼k'], bins=30, kde=True, color='salmon')
plt.title('Log10 DÃ¶nÃ¼ÅŸÃ¼mlÃ¼ DaÄŸÄ±lÄ±m\n(logâ‚â‚€(Richter + 0.01))')
plt.xlabel('Log10(BÃ¼yÃ¼klÃ¼k)')

plt.tight_layout()
plt.show()

# 1. BÃ¼yÃ¼klÃ¼k kategorileri oluÅŸturma
df['BÃ¼yÃ¼klÃ¼k_Kategori'] = pd.cut(df['BÃ¼yÃ¼klÃ¼k'],
                                bins=[0, 3, 5, np.inf],
                                labels=['Hafif', 'Orta', 'Åiddetli'])

# 2. Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼ + standardizasyon
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df['Log_BÃ¼yÃ¼klÃ¼k_Std'] = scaler.fit_transform(df[['Log_BÃ¼yÃ¼klÃ¼k']])

plt.figure(figsize=(16, 6))

# Orijinal daÄŸÄ±lÄ±m
plt.subplot(1, 3, 1)
sns.violinplot(x=df['BÃ¼yÃ¼klÃ¼k'], color='lightblue')
plt.title('Orijinal BÃ¼yÃ¼klÃ¼k DaÄŸÄ±lÄ±mÄ±\n(Richter Ã–lÃ§eÄŸi)')

# Log dÃ¶nÃ¼ÅŸÃ¼mlÃ¼ daÄŸÄ±lÄ±m
plt.subplot(1, 3, 2)
sns.violinplot(x=df['Log_BÃ¼yÃ¼klÃ¼k'], color='lightgreen')
plt.title('Log10 DÃ¶nÃ¼ÅŸÃ¼mlÃ¼ DaÄŸÄ±lÄ±m')

# KÃ¼mÃ¼latif daÄŸÄ±lÄ±m
plt.subplot(1, 3, 3)
sns.ecdfplot(data=df, x='BÃ¼yÃ¼klÃ¼k', complementary=True)
plt.yscale('log')
plt.title('KÃ¼mÃ¼latif DaÄŸÄ±lÄ±m (Log-Log)')
plt.tight_layout()

from geopy.distance import geodesic

def label_aftershocks(df, mainshock_mag_threshold=6.0, time_window_days=7, distance_km=50):
    df = df.sort_values("OluÅŸ ZamanÄ±").reset_index(drop=True)
    df['is_aftershock'] = 0  # BaÅŸlangÄ±Ã§ta tÃ¼m satÄ±rlar artÃ§Ä± deÄŸil

    for i, row in df.iterrows():
        if row['BÃ¼yÃ¼klÃ¼k'] >= mainshock_mag_threshold:
            mainshock_time = row['OluÅŸ ZamanÄ±']
            mainshock_loc = (row['Enlem'], row['Boylam'])

            time_window = (df['OluÅŸ ZamanÄ±'] > mainshock_time) & \
                          (df['OluÅŸ ZamanÄ±'] <= mainshock_time + pd.Timedelta(days=time_window_days))

            for j in df[time_window].index:
                aftershock_loc = (df.loc[j, 'Enlem'], df.loc[j, 'Boylam'])
                distance = geodesic(mainshock_loc, aftershock_loc).km

                if distance <= distance_km and df.loc[j, 'BÃ¼yÃ¼klÃ¼k'] < row['BÃ¼yÃ¼klÃ¼k']:
                    df.at[j, 'is_aftershock'] = 1

    return df

df = label_aftershocks(df)
df['is_aftershock'].value_counts()

from geopy.distance import geodesic

def ozellikleri_olustur(df):
    df = df.sort_values("OluÅŸ ZamanÄ±").reset_index(drop=True)

    df['Unix ZamanÄ±'] = df['OluÅŸ ZamanÄ±'].astype(np.int64) // 10**9
    df['Saat'] = df['OluÅŸ ZamanÄ±'].dt.hour
    df['HaftanÄ±n GÃ¼nÃ¼'] = df['OluÅŸ ZamanÄ±'].dt.dayofweek

    zaman_farki = [0]
    mesafe_farki = [0]

    for i in range(1, len(df)):
        simdi_zaman = df.loc[i, 'OluÅŸ ZamanÄ±']
        onceki_zaman = df.loc[i-1, 'OluÅŸ ZamanÄ±']
        zaman_farki.append((simdi_zaman - onceki_zaman).total_seconds())

        simdi_konum = (df.loc[i, 'Enlem'], df.loc[i, 'Boylam'])
        onceki_konum = (df.loc[i-1, 'Enlem'], df.loc[i-1, 'Boylam'])
        mesafe_farki.append(geodesic(simdi_konum, onceki_konum).km)

    df['Ã–nceki Zaman FarkÄ± (sn)'] = zaman_farki
    df['Ã–nceki Mesafe (km)'] = mesafe_farki

    return df

df = ozellikleri_olustur(df)
df[['BÃ¼yÃ¼klÃ¼k', 'Derinlik', 'Saat', 'HaftanÄ±n GÃ¼nÃ¼', 'Ã–nceki Zaman FarkÄ± (sn)', 'Ã–nceki Mesafe (km)']].head()

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

ozellikler = ['BÃ¼yÃ¼klÃ¼k', 'Derinlik', 'Saat', 'HaftanÄ±n GÃ¼nÃ¼',
              'Ã–nceki Zaman FarkÄ± (sn)', 'Ã–nceki Mesafe (km)']

X = df[ozellikler]
y = df['is_aftershock']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

model = RandomForestClassifier(random_state=42, class_weight='balanced')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, accuracy_score

def modeli_test_et(model, X_train, y_train, X_test, y_test, model_adi):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    print(f"ğŸ”¹ {model_adi} SonuÃ§larÄ±:")
    print("Accuracy:", round(accuracy_score(y_test, y_pred), 3))
    print(classification_report(y_test, y_pred, target_names=["ArtÃ§Ä± DeÄŸil", "ArtÃ§Ä±"]))
    print("-" * 60)

modeller = {
    "Random Forest": RandomForestClassifier(random_state=42, class_weight='balanced'),
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "XGBoost": XGBClassifier(random_state=42, scale_pos_weight=12),  # sÄ±nÄ±f dengesizliÄŸi iÃ§in
    "LightGBM": LGBMClassifier(random_state=42, class_weight='balanced')
}

for isim, model in modeller.items():
    modeli_test_et(model, X_train, y_train, X_test, y_test, isim)

import joblib
from xgboost import XGBClassifier # XGBoost sÄ±nÄ±fÄ±nÄ± yeniden import et

# Modelleri test etme dÃ¶ngÃ¼sÃ¼ zaten Ã§alÄ±ÅŸtÄ±.
# Bu dÃ¶ngÃ¼nÃ¼n sonunda `model` deÄŸiÅŸkeni en son test edilen modeli (LightGBM) iÃ§erir.
# XGBoost modelinin eÄŸitilmiÅŸ halini kaydetmek istiyorsanÄ±z,
# ya dÃ¶ngÃ¼ iÃ§inde kaydedin ya da dÃ¶ngÃ¼ dÄ±ÅŸÄ±nda XGBoost modeline tekrar eriÅŸin.

# En basit Ã§Ã¶zÃ¼m, dÃ¶ngÃ¼ bittikten sonra XGBoost modeline tekrar eriÅŸmek ve kaydetmektir.
# Modeller sÃ¶zlÃ¼ÄŸÃ¼nden XGBoost modelini alÄ±n
xgboost_model_instance = modeller["XGBoost"]

# modeli_test_et fonksiyonu her dÃ¶ngÃ¼de modeli eÄŸitir.
# DÃ¶ngÃ¼ zaten tamamlandÄ±ÄŸÄ± iÃ§in, `xgboost_model_instance` deÄŸiÅŸkeni eÄŸitilmiÅŸ XGBoost modelini tutar.

# Åimdi bu eÄŸitilmiÅŸ XGBoost modelini kaydedin
joblib.dump(xgboost_model_instance, 'xgboost_smote_model.pkl') # DoÄŸru deÄŸiÅŸken adÄ± kullanÄ±ldÄ±

print("XGBoost modeli baÅŸarÄ±yla 'xgboost_smote_model.pkl' dosyasÄ±na kaydedildi.")

# Ä°stediÄŸin zaman geri yÃ¼klemek iÃ§in:
# loaded_model = joblib.load('xgboost_smote_model.pkl')

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import pandas as pd # pandas'Ä± tekrar import et

# Ã–zellik ve hedef deÄŸiÅŸkenleri ayÄ±r
ozellikler = ['BÃ¼yÃ¼klÃ¼k', 'Derinlik', 'Saat', 'HaftanÄ±n GÃ¼nÃ¼',
              'Ã–nceki Zaman FarkÄ± (sn)', 'Ã–nceki Mesafe (km)'] # Ã–zellik listesini yeniden tanÄ±mla

# Sadece kullanÄ±lacak sÃ¼tunlarda eksik deÄŸer kontrolÃ¼ yap
df_cleaned = df.dropna(subset=ozellikler + ['is_aftershock']).copy() # Eksik deÄŸer iÃ§eren satÄ±rlarÄ± at ve kopya oluÅŸtur

# TemizlenmiÅŸ DataFrame'den Ã¶zellikleri ve hedef deÄŸiÅŸkeni ayÄ±r
X = df_cleaned[ozellikler]
y = df_cleaned["is_aftershock"]

# Veriyi eÄŸitim ve test olarak ayÄ±r
# stratify=y, sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± korumak iÃ§in hala Ã¶nemlidir
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# SMOTE uygulama
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± kontrol edelim
print("Ã–nce (EÄŸitim Seti):", y_train.value_counts())
print("Sonra (SMOTE SonrasÄ± EÄŸitim Seti):", y_train_resampled.value_counts())

# Model eÄŸitimi ve deÄŸerlendirmesi iÃ§in XGBoost ve diÄŸer modeller
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix

params = {
    "n_estimators": [100, 200],
    "max_depth": [3, 5, 7],
    "learning_rate": [0.01, 0.1, 0.2],
    "subsample": [0.7, 1.0],
    "colsample_bytree": [0.7, 1.0]
}

# XGBoost modelini SMOTE sonrasÄ± eÄŸitim verisiyle eÄŸit
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

grid = GridSearchCV(xgb, params, scoring='f1', cv=3, n_jobs=-1)
grid.fit(X_train_resampled, y_train_resampled)

print("En iyi parametreler:", grid.best_params_)

best_model = grid.best_estimator_
y_pred = best_model.predict(X_test)

print("\nGridSearchCV (SMOTE + XGBoost) SonuÃ§larÄ± (Test Seti):")
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# DiÄŸer modelleri de SMOTE sonrasÄ± eÄŸitim verisiyle test etmek isterseniz:
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score

# Daha Ã¶nce tanÄ±mlanmÄ±ÅŸ modeli_test_et fonksiyonunu kullanabilirsiniz
# Ancak bu fonksiyon SMOTE'yi otomatik yapmÄ±yor, bu yÃ¼zden manuel olarak
# X_train_resampled ve y_train_resampled ile modelleri yeniden eÄŸitmeniz gerekir.

print("\nDiÄŸer Modellerin SonuÃ§larÄ± (SMOTE SonrasÄ± EÄŸitim ile):")
modeller_smote = {
    "Random Forest": RandomForestClassifier(random_state=42, class_weight='balanced'),
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "KNN": KNeighborsClassifier(n_neighbors=5), # KNN class_weight'i desteklemez, scale_pos_weight XGBoost/LightGBM'e Ã¶zgÃ¼dÃ¼r
    "LightGBM": LGBMClassifier(random_state=42, class_weight='balanced')
}

for isim, model in modeller_smote.items():
    model.fit(X_train_resampled, y_train_resampled) # Modeli SMOTElenmiÅŸ veriyle eÄŸit
    y_pred_other = model.predict(X_test)
    print(f"ğŸ”¹ {isim} SonuÃ§larÄ± (SMOTE EÄŸitim):")
    print("Accuracy:", round(accuracy_score(y_test, y_pred_other), 3))
    print(classification_report(y_test, y_pred_other, target_names=["ArtÃ§Ä± DeÄŸil", "ArtÃ§Ä±"]))
    print("-" * 60)

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Temel XGBoost modeli
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train_resampled, y_train_resampled)

# Test verisinde tahmin
y_pred_xgb = xgb.predict(X_test)

# SonuÃ§lar
print("ğŸ”¹ XGBoost SonuÃ§larÄ± (SMOTE EÄŸitim):")
print(classification_report(y_test, y_pred_xgb, target_names=["ArtÃ§Ä± DeÄŸil", "ArtÃ§Ä±"]))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
grid = GridSearchCV(estimator=xgb_model,
                    param_grid=param_grid,
                    scoring='f1',
                    cv=3,
                    n_jobs=-1,
                    verbose=1)

grid.fit(X_train_resampled, y_train_resampled)

from sklearn.metrics import classification_report, confusion_matrix

best_xgb = grid.best_estimator_
y_pred_best = best_xgb.predict(X_test)

print("ğŸ”§ En iyi parametreler:", grid.best_params_)
print("ğŸ”¹ XGBoost (GridSearch) SonuÃ§larÄ±:")
print(classification_report(y_test, y_pred_best, target_names=["ArtÃ§Ä± DeÄŸil", "ArtÃ§Ä±"]))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_best))

from sklearn.metrics import classification_report, confusion_matrix

y_proba = best_xgb.predict_proba(X_test)[:, 1]

for thresh in [0.5, 0.4, 0.35, 0.3]:
    print(f"\nğŸ¯ EÅŸik: {thresh}")
    y_pred_thresh = (y_proba >= thresh).astype(int)
    print(classification_report(y_test, y_pred_thresh, target_names=["ArtÃ§Ä± DeÄŸil", "ArtÃ§Ä±"]))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_thresh))
